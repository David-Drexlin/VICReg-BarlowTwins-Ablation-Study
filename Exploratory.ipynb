{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e48d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "from view_transform import ViewTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb1838",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a9a44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO\\n- We use a learning rate warm-up period of 10 epochs, after which we reduce the learning rate by a factor of 1000 using a co- sine decay schedule \\n- LARS Optimizer there are implementations on github etc. \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TODO\n",
    "- We use a learning rate warm-up period of 10 epochs, after which we reduce the learning rate by a factor of 1000 using a co- sine decay schedule \n",
    "- LARS Optimizer there are implementations on github etc. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ede9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "epochs = 1 # Original set to 1000 \n",
    "dim = 1000 # depends on specific encoder architecture ie. modifications of basic, as-is ResNet50\n",
    "\n",
    "batch_size = 16 \n",
    "num_workers = 4\n",
    "device = 'cpu' # or 'cuda' for faster training\n",
    "\n",
    "# VicREG\n",
    "base_lr = 0.2\n",
    "learning_rate = batch_size/256 * base_lr #  for barlow twins \n",
    "weight_decay = 1e-6\n",
    "\n",
    "# BarlowTwins\n",
    "# learning_rate = base_lr * batch_size / 256\n",
    "# weight_decay = 1.5*1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79dd934",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445121ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "        ])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54b6deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())  \n",
    "trainset.transform = ViewTransform()\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "linear_trainset = trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=linear_transform)  \n",
    "linear_trainloader = torch.utils.data.DataLoader(linear_trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=linear_transform)  \n",
    "testnset_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09abaec",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f25e24aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = torchvision.models.resnet50()\n",
    "\n",
    "def projector():\n",
    "    proj_layers = []\n",
    "\n",
    "    for i in range(4):\n",
    "        proj_layers.append(torch.nn.Linear(dim, dim))\n",
    "        proj_layers.append(torch.nn.ReLU(dim))\n",
    "        proj_layers.append(torch.nn.BatchNorm1d(dim))\n",
    "    \n",
    "    return torch.nn.Sequential(*proj_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d8a5c7",
   "metadata": {},
   "source": [
    "## VicREG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0f33d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VicReg Paper - with modifications\n",
    "def VIC_Reg(z1, z2):\n",
    "    \n",
    "    N = z1.shape[0]\n",
    "    D = z1.shape[1]\n",
    "\n",
    "    mu = 25 # as per VIC-Reg Paper -- Subject to change\n",
    "    la = 25 # \"   \"\n",
    "    nu = 1 #  \"   \"\n",
    "    \n",
    "    # invariance loss\n",
    "    sim_loss = F.mse_loss(z1, z2)\n",
    "    \n",
    "    # variance loss\n",
    "    std_z_a = torch.sqrt(z1.var(dim=0) + 1e-04)\n",
    "    std_z_b = torch.sqrt(z2.var(dim=0) + 1e-04)\n",
    "    std_loss = torch.mean(torch.relu(1 - std_z_a)) + torch.mean(torch.relu(1 - std_z_b))\n",
    "    \n",
    "    # covariance loss\n",
    "    z1 = z1 - z1.mean(dim=0)\n",
    "    z2 = z2 - z2.mean(dim=0)\n",
    "    \n",
    "    cov_z_a = (z1.T @ z1) / (N - 1)\n",
    "    cov_z_b = (z2.T @ z2) / (N - 1)\n",
    "\n",
    "    cov_z_a = cov_z_a[~torch.eye(cov_z_a.shape[0], dtype=bool)] # Off diags\n",
    "    cov_z_b = cov_z_b[~torch.eye(cov_z_b.shape[0], dtype=bool)] # \" \"\n",
    "\n",
    "    cov_loss = cov_z_a.pow_(2).sum() / D + cov_z_b.pow_(2).sum() / D\n",
    "    loss = la * sim_loss + mu * std_loss + nu * cov_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d35eb",
   "metadata": {},
   "source": [
    "## Barlow Twins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0eb0850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barlow Twins Paper - with modifications\n",
    "\n",
    "def barlow_twins(z1, z2):\n",
    "    N = z1.shape[0]\n",
    "    D = z1.shape[1]\n",
    "    \n",
    "    z1_norm = (z1 - z1.mean(0)) / z1.std(0) # NxD\n",
    "    z2_norm = (z2 - z2.mean(0)) / z2.std(0) # NxD\n",
    "\n",
    "    z1_norm = z1_norm[:, None , :]  \n",
    "    z2_norm = z2_norm[:, None , :] \n",
    "\n",
    "    c = F.conv1d(z1_norm.T, z2_norm.T)\n",
    "\n",
    "    \n",
    "    # loss\n",
    "    c_diff = (c - torch.eye(D)).pow(2) # DxD #multiplyoff-diagonalelemsofc_diffbylambda off_diagonal(c_diff).mul_(lambda)\n",
    "    loss = c_diff.sum()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96ecc280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 1000])\n",
      "torch.Size([1000, 1000, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.5011e+10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barlow_twins(torch.rand((16,1000)), torch.rand((16,1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ccb0d4",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce6b4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainset, loss_mode = \"VicReg\"): \n",
    "    \n",
    "    mode = loss_mode ==  \"VicReg\"\n",
    "\n",
    "    encoder = torchvision.models.resnet50()\n",
    "    rpoj = projector()\n",
    "    model = torch.nn.Sequential(encoder, rpoj)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # TODO LARS\n",
    "\n",
    "    for i in range(epochs):\n",
    "        losses = []\n",
    "        for (x1, x2), _ in tqdm(trainloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "            z1, z2 = model(x1), model(x2)\n",
    "\n",
    "            if mode: \n",
    "                loss = VIC_Reg(z1, z2)\n",
    "            else:\n",
    "                loss = barlow_twins(z1, z2)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.detach().item())\n",
    "\n",
    "\n",
    "        print(f\"Epoch: {i}, loss: {np.mean(losses)}\")\n",
    "        #DL 1 Homework 1 \n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        if mode:\n",
    "            os.makedirs('VicReg', exist_ok=True)\n",
    "            torch.save(encoder.state_dict(), f'models/VicReg/model_{batch_size}_epoch_{i}.pt')\n",
    "        else:\n",
    "            os.makedirs('BarlowTwins', exist_ok=True)\n",
    "            torch.save(encoder.state_dict(), f'models/BarlowTwins/model_{batch_size}_epoch_{i}.pt')\n",
    "\n",
    "    \n",
    "    return encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5de703bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [44:56<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 39.53907440063477\n"
     ]
    }
   ],
   "source": [
    "encoder_vicreg = train(trainloader, loss_mode=\"VicReg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6720257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [45:47<00:00,  1.14it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 39.946143161621094\n"
     ]
    }
   ],
   "source": [
    "encoder_barlow = train(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99644a1e",
   "metadata": {},
   "source": [
    "## Linear Head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e940b3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_vicreg.eval()\n",
    "encoder_barlow.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31ab661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path duplicate - keep for now as convenience loader\n",
    "def load_models(path=\"models/VicReg/model_16_epoch_0.pt\"):\n",
    "    encoder = torchvision.models.resnet50()\n",
    "    saved = torch.load(path)\n",
    "    encoder.load_state_dict(saved)\n",
    "    return encoder\n",
    "\n",
    "def linear_train(path=\"models/VicReg/model_16_epoch_0.pt\"):\n",
    "    \n",
    "    encoder = load_models(path)\n",
    "    encoder.eval()\n",
    "\n",
    "    # I dont understand how the guys from the vicreg paper combinded \n",
    "    # this LinearSVC and pytorch, to my understanding indicated\n",
    "    # by the fact that they use optimiser (but I am no expert, by any means) ^^\n",
    "\n",
    "    clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "\n",
    "    model = torch.nn.Sequential(encoder, clf)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        losses = []\n",
    "        for x, y in tqdm(linear_trainloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            \n",
    "            loss = torch.nn.MSELoss(output, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.detach().item())\n",
    "\n",
    "    \n",
    "    print(f\"Epoch: {i}, loss: {np.mean(losses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cca46dff",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sklearn.svm._classes.LinearSVC is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m linear_train()\n",
      "Cell \u001b[0;32mIn[55], line 14\u001b[0m, in \u001b[0;36mlinear_train\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     11\u001b[0m encoder\u001b[39m.\u001b[39meval()\n\u001b[1;32m     12\u001b[0m clf \u001b[39m=\u001b[39m LinearSVC(random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, tol\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mSequential(encoder, clf)\n\u001b[1;32m     15\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate, weight_decay\u001b[39m=\u001b[39mweight_decay)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:91\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[39mfor\u001b[39;00m idx, module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(args):\n\u001b[0;32m---> 91\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_module(\u001b[39mstr\u001b[39;49m(idx), module)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:444\u001b[0m, in \u001b[0;36mModule.add_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Adds a child module to the current module.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \u001b[39mThe module can be accessed as an attribute using the given name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39m    module (Module): child module to be added to the module.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(module, Module) \u001b[39mand\u001b[39;00m module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not a Module subclass\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    445\u001b[0m         torch\u001b[39m.\u001b[39mtypename(module)))\n\u001b[1;32m    446\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(name, torch\u001b[39m.\u001b[39m_six\u001b[39m.\u001b[39mstring_classes):\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule name should be a string. Got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    448\u001b[0m         torch\u001b[39m.\u001b[39mtypename(name)))\n",
      "\u001b[0;31mTypeError\u001b[0m: sklearn.svm._classes.LinearSVC is not a Module subclass"
     ]
    }
   ],
   "source": [
    "linear_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784a78c",
   "metadata": {},
   "source": [
    "\n",
    "Linear classification. We follow standard protocols Misra & Maaten (2020); Caron et al. (2020); Zbontar et al. (2021) and train linear models on top of the frozen representations. For VOC07 Everingham et al. (2010), we train a linear SVM with LIBLINEAR Fan et al. (2008). The images are center cropped and resized to 224 × 224, and the C values are computed with cross-validation. For Places205 Zhou et al. (2014) we use SGD with a learning rate of 0.003, a weight decay of 0.0001, a momentum of 0.9 and a batch size of 256, for 28 epochs. The learning rate is divided by 10 at epochs 4, 8 and 12. For Inaturalist2018 Horn et al. (2018), we use SGD with a learning rate of 0.005, a weight decay of 0.0001, a momentum of 0.9 and a batch size of 256, for 84 epochs. The learning rate is divided by 10 at epochs 24, 48 and 72."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_train(\"models/BarlowTwins/model_16_epoch_0.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
