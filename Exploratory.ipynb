{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e48d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import glob \n",
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from view_transform import ViewTransform\n",
    "from LARS import LARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb1838",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a9a44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO\\n-->> It might be interesting investigate the efficiency frontier between max_batch and views \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TODO\n",
    "-->> It might be interesting investigate the efficiency frontier between max_batch and views \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ede9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "warmup_steps = 10 \n",
    "start_epoch = 1\n",
    "epochs = 100 # Original set to 1000 \n",
    "output_enc = 512\n",
    "dim = 8192\n",
    "num_views = 4\n",
    "offset = 1 \n",
    "\n",
    "num_workers = 2\n",
    "device = 'cuda' # or 'cuda' for faster training\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "# VicREG\n",
    "base_lr_ = 0.2\n",
    "lr_head = 0.3\n",
    "learning_rate = batch_size/256 * base_lr_ \n",
    "weight_decay = 1e-6\n",
    "\n",
    "logging.basicConfig(filename=f'logs/b:{batch_size}.log', filemode='w', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79dd934",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54b6deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "#trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)  \n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)  \n",
    "trainset.transform = ViewTransform(2)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09abaec",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f25e24aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projector():\n",
    "    proj_layers = []\n",
    "    proj_layers.append(torch.nn.Flatten())\n",
    "\n",
    "    proj_layers.append(torch.nn.Linear(output_enc, dim))\n",
    "    proj_layers.append(torch.nn.ReLU(dim))\n",
    "    proj_layers.append(torch.nn.BatchNorm1d(dim))\n",
    "\n",
    "    proj_layers.append(torch.nn.Linear(dim, dim))\n",
    "    proj_layers.append(torch.nn.ReLU(dim))\n",
    "    proj_layers.append(torch.nn.BatchNorm1d(dim))\n",
    "    \n",
    "    proj_layers.append(torch.nn.Linear(dim, dim, bias=False))\n",
    "    \n",
    "    return torch.nn.Sequential(*proj_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d8a5c7",
   "metadata": {},
   "source": [
    "## VicREG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0f33d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VicReg Paper - with modifications\n",
    "def VIC_Reg(Z):\n",
    "    \n",
    "    N = Z[0].shape[0]\n",
    "    D = Z[0].shape[1]\n",
    "\n",
    "    mu = 25\n",
    "    la = 25\n",
    "    nu = 1 \n",
    "    \n",
    "    sim_loss = 0\n",
    "    std_loss = 0\n",
    "    cov_loss = 0\n",
    "\n",
    "    for i in range(len(Z)): \n",
    "        for j in range(i+1, len(Z)): \n",
    "            sim_loss += F.mse_loss(Z[i], Z[j])\n",
    "\n",
    "    for zi in Z: \n",
    "        std_zi = torch.sqrt(zi.var(dim=0) + 1e-04)\n",
    "        std_loss += torch.mean(torch.relu(1 - std_zi)) \n",
    "    \n",
    "    for zi in Z: \n",
    "        zi = zi - zi.mean(dim=0)\n",
    "        cov_zi = (zi.T @ zi) / (N - 1)\n",
    "        cov_zi = cov_zi[~torch.eye(cov_zi.shape[0], dtype=bool,device=device)]\n",
    "        cov_loss += cov_zi.pow_(2).sum() / D\n",
    "\n",
    "    sim_loss /= (len(Z) * (len(Z)-1)) / 2\n",
    "    std_loss /= len(Z)\n",
    "    cov_loss /= len(Z)\n",
    "    \n",
    "    logging.info('IL: %.3f, STDL: %.3f, CVL: %.3f',la * sim_loss, mu * std_loss, nu * cov_loss)\n",
    "\n",
    "    loss = la * sim_loss + mu * std_loss + nu * cov_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e95f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
    "\n",
    "def adjust_learning_rate(optimizer, loader, step):\n",
    "    max_steps = epochs * len(loader)\n",
    "    warmup_steps = 10 * len(loader)\n",
    "    base_lr = base_lr_ * batch_size / 256\n",
    "    if step < warmup_steps:\n",
    "        lr = base_lr * step / warmup_steps\n",
    "    else:\n",
    "        step -= warmup_steps\n",
    "        max_steps -= warmup_steps\n",
    "        q = 0.5 * (1 + math.cos(math.pi * step / max_steps))\n",
    "        end_lr = base_lr * 0.001\n",
    "        lr = base_lr * q + end_lr * (1 - q)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ccb0d4",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6b4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainset, offset, batch_size=2048, load_from_checkpoint=\"\"): \n",
    "\n",
    "    encoder = torchvision.models.resnet18() # also try with pretrained=true\n",
    "    encoder.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    encoder = torch.nn.Sequential(*list(encoder.children())[:-1])\n",
    "\n",
    "    \n",
    "    rpoj = projector()\n",
    "    model = torch.nn.Sequential(encoder, rpoj)\n",
    "    model.train()\n",
    "    optimizer = LARS(model.parameters(),lr=0,weight_decay=weight_decay)\n",
    "    \n",
    "    if load_from_checkpoint != \"\": \n",
    "        checkpoint = torch.load(load_from_checkpoint)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        #start_epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss'] \n",
    "        model.train()\n",
    "        \n",
    "    model = model.to(device)\n",
    "    last_time = time.time()\n",
    "\n",
    "    for i in range(start_epoch, epochs+1):\n",
    "        losses = []\n",
    "        total_loss = 0\n",
    "        for step, (X, _) in tqdm(enumerate(trainloader), total=epochs):\n",
    "            \n",
    "            Z = []\n",
    "            for xi in X: \n",
    "                xi = xi.to(device)\n",
    "                Z.append(model(xi))\n",
    "                \n",
    "            loss = VIC_Reg(Z) / offset # Why? \n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if ((step+1) % offset == 0) or ((step+1)==len(trainloader)): \n",
    "                lr = adjust_learning_rate(optimizer, trainloader, (step+1)/offset) \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                losses.append(total_loss)\n",
    "                logging.info('%s ,Epoch: %d, Step: %d, Loss: %.3f, Elapsed: %d, View: %d', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())), i, step, total_loss, time.time()-last_time, len(Z))\n",
    "                total_loss = 0 \n",
    "\n",
    "            last_time = time.time()\n",
    "\n",
    "\n",
    "        print(f\"Epoch: {i}, loss: {np.mean(losses)}\")\n",
    "        #DL 1 Homework 1 \n",
    "\n",
    "        if i%10==0:\n",
    "            torch.save({\n",
    "                'epoch': i,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': np.mean(losses),\n",
    "                }, f'model_{num_views}_{batch_size}_epoch_{i}.pt')\n",
    "    \n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff997348",
   "metadata": {},
   "source": [
    "### Fixed Batch - Increasing Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c08dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View: 2 / 4\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [03:08<5:10:21, 188.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m         \u001b[39m#if(at_limit):\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         \u001b[39m#    batch_size //= 2\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         \u001b[39m#    offset = offset * 2\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[39m#    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m         encoder_vicreg \u001b[39m=\u001b[39m train(trainloader, offset)\n\u001b[1;32m     23\u001b[0m         \u001b[39mbreak\u001b[39;00m  \u001b[39m#  successful break out of the loop\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(trainset, offset, batch_size, load_from_checkpoint)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m xi \u001b[39min\u001b[39;00m X: \n\u001b[1;32m     35\u001b[0m     xi \u001b[39m=\u001b[39m xi\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 36\u001b[0m     Z\u001b[39m.\u001b[39mappend(model(xi))\n\u001b[1;32m     38\u001b[0m loss \u001b[39m=\u001b[39m VIC_Reg(Z)\n\u001b[1;32m     39\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in [2,4,6,8]:\n",
    "    print(f\"View: {i} / {num_views}\")\n",
    "    view = i \n",
    "    at_limit = False\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)  \n",
    "    trainset.transform = ViewTransform(view)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    #\n",
    "    #logging.basicConfig(filename=f'logs/loss_details_b:{batch_size}.log', filemode='w', level=logging.DEBUG)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            #if(at_limit):\n",
    "            #    batch_size //= 2\n",
    "            #    offset = offset * 2\n",
    "            #    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "                \n",
    "            encoder_vicreg = train(trainloader, offset)\n",
    "            break  #  successful break out of the loop\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(\"Out of memory error occurred. Reducing batch size and retrying...\")\n",
    "                # Reduce batch size & Conversly increase step size\n",
    "                batch_size //= 2\n",
    "                offset = offset * 2\n",
    "                at_limit = True\n",
    "\n",
    "                trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a20f8d",
   "metadata": {},
   "source": [
    "### Fixed Total Gradients - Increase Views & Reduce Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5294e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [4,6,8]:\n",
    "    print(f\"View: {i} / {num_views}\")\n",
    "    view = i \n",
    "    at_limit = False\n",
    "    \n",
    "    batch_size = 2 * batch_size / i \n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)  \n",
    "    trainset.transform = ViewTransform(view)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    #\n",
    "    #logging.basicConfig(filename=f'logs/loss_details_b:{batch_size}.log', filemode='w', level=logging.DEBUG)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            #if(at_limit):\n",
    "            #    batch_size //= 2\n",
    "            #    offset = offset * 2\n",
    "            #    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "                \n",
    "            encoder_vicreg = train(trainloader, offset, batch_size)\n",
    "            break  #  successful break out of the loop\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(\"Out of memory error occurred. Reducing batch size and retrying...\")\n",
    "                # Reduce batch size & Conversly increase step size\n",
    "                batch_size //= 2\n",
    "                offset = offset * 2\n",
    "                at_limit = True\n",
    "\n",
    "                trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea06eb0",
   "metadata": {},
   "source": [
    "## Model Evaluations - Beyond Loss (kNN, LinearHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a33392",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(32),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models(path=\"./models/\"):\n",
    "    return glob.glob(path+\"*.pt\")\n",
    "\n",
    "def load_final_models(path=\"./models/\"):\n",
    "    return glob.glob(path+\"*100*.pt\")\n",
    "# ChatGPT &/ PyTorch topK \n",
    "def top_k_accuracy(output, target, k=1):\n",
    "    _, pred = output.topk(k, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    correct_k = correct[:k].reshape(-1).float().sum()\n",
    "    accuracy = correct_k.mul_(100.0 / target.size(0))\n",
    "    return accuracy.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_path in tqdm(load_all_models(), total=len(load_all_models())): \n",
    "\n",
    "    encoder = torchvision.models.resnet18()\n",
    "    encoder.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    encoder = torch.nn.Sequential(*list(encoder.children())[:-1])\n",
    "\n",
    "    checkpoint = torch.load(model_path)\n",
    "    encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    encoder.eval()\n",
    "    encoder.to(device)\n",
    "    \n",
    "    X_train_embedding = []\n",
    "    X_test_embedding = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "\n",
    "    for x,y in train_loader:\n",
    "        x = x.to(device)\n",
    "        X_train_embedding.append(encoder(x))\n",
    "        y_train.append(y_train)\n",
    "\n",
    "    for x,y in test_loader:\n",
    "        x = x.to(device)\n",
    "        X_test_embedding.append(encoder(x))\n",
    "        y_test.append(y_train)\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train_embedding, y_train)\n",
    "    X_test_predicted = knn.predict(X_test_embedding)\n",
    "    accuracy = accuracy_score(y_test, X_test_predicted)\n",
    "    logging.info(\"Model: %s, kNN-Accuracy: %.3f\", model_path, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_path in tqdm(load_final_models(), total=epochs*len(load_final_models())):\n",
    "    \n",
    "    encoder = torchvision.models.resnet18()\n",
    "    encoder.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    encoder.requires_grad_(False)\n",
    "    encoder.eval()\n",
    "    encoder.to(device)\n",
    "\n",
    "    head = torch.nn.Linear(output_enc, num_classes)\n",
    "    head.weight.data.normal_(mean=0.0, std=0.01)\n",
    "    head.bias.data.zero_()\n",
    "    model = torch.nn.Sequential(encoder, head)\n",
    "\n",
    "    param_groups = [dict(params=head.parameters(), lr=lr_head)]\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.SGD(param_groups, 0, momentum=0.9, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        for step, (X, Y) in tqdm(enumerate(trainloader), total=epochs):\n",
    "            X = X.to(device)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                accuracy = []\n",
    "                for images, targets in test_loader: \n",
    "                    output = model(images)\n",
    "                    accuracy.append(top_k_accuracy(output, targets))\n",
    "                \n",
    "                accuracy = accuracy.mean()\n",
    "            \n",
    "            logging.info(\"Model: %s, Accuracy: %.3f, Epoch: %d\", model_path, accuracy, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68c1bd",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0693a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"logs/b_2048_v_2.log\", delimiter=',', header=None)\n",
    "columns = [\"Info\", \"Epoch\", \"Step\", \"Loss\", \"Elapsed\", \"Views\"]\n",
    "df.columns = columns\n",
    "df[\"Epoch\"] = df[\"Epoch\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "df[\"Step\"] = df[\"Step\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "df[\"Loss\"] = df[\"Loss\"].str.extract(r\"(\\d+\\.\\d+)\", expand=False).astype(float)\n",
    "df[\"Elapsed\"] = df[\"Elapsed\"].str.extract(r\"(\\d+)\", expand=False).astype(float)\n",
    "df[\"Views\"] = df[\"Views\"].str.extract(r\"(\\d+)\", expand=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fc2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    grouped_data = df[df[\"Views\"] == i+2]\n",
    "    #grouped_data = grouped_data.groupby(\"Epoch\").agg({\"Loss\": \"mean\"}).reset_index()\n",
    "    ax.plot(grouped_data[\"Loss\"])\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"View {i+2}\")\n",
    "\n",
    "for i in range(2,num_views+1): \n",
    "    grouped_data = df[df[\"Views\"] == i]\n",
    "    #grouped_data.groupby(\"Epoch\").agg({\"Loss\": \"mean\"}).reset_index()\n",
    "    ax.plot(grouped_data[\"Epoch\"], grouped_data[\"Loss\"],label=f\"{i} Views\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Average Loss\")\n",
    "\n",
    "\n",
    "# Get the handles and labels from the plots\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.figlegend(handles, labels, loc='center', bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "for view in range(2,num_views+1):\n",
    "    last_loss = df[df[\"Views\"] == view][\"Loss\"].tail(1).values[0]\n",
    "    print(f\"Last loss for View {view}: {last_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b0c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"logs/b_128_v_2.log\", delimiter=',', header=None)\n",
    "columns = [\"Info\", \"Epoch\", \"Step\", \"Loss\", \"Elapsed\", \"Views\"]\n",
    "df.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"logs/b_128_v_2.log\", delimiter=',', header=None)\n",
    "columns = [\"Info\", \"Epoch\", \"Step\", \"Loss\", \"Elapsed\", \"Views\"]\n",
    "df.columns = columns\n",
    "df[\"Epoch\"] = df[\"Epoch\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "df[\"Step\"] = df[\"Step\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "df[\"Loss\"] = df[\"Loss\"].str.extract(r\"(\\d+\\.\\d+)\", expand=False).astype(float)\n",
    "df[\"Elapsed\"] = df[\"Elapsed\"].str.extract(r\"(\\d+)\", expand=False).astype(float)\n",
    "df[\"Views\"] = df[\"Views\"].str.extract(r\"(\\d+)\", expand=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fcf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    grouped_data = df[df[\"Views\"] == i+2]\n",
    "    #grouped_data = grouped_data.groupby(\"Epoch\").agg({\"Loss\": \"mean\"}).reset_index()\n",
    "    ax.plot(grouped_data[\"Loss\"])\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"View {i+2}\")\n",
    "\n",
    "for i in range(2,3+1): \n",
    "    grouped_data = df[df[\"Views\"] == i]\n",
    "    #grouped_data.groupby(\"Epoch\").agg({\"Loss\": \"mean\"}).reset_index()\n",
    "    ax.plot(grouped_data[\"Epoch\"], grouped_data[\"Loss\"],label=f\"{i} Views\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Average Loss\")\n",
    "\n",
    "\n",
    "# Get the handles and labels from the plots\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.figlegend(handles, labels, loc='center', bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
