{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e48d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import glob \n",
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from view_transform import ViewTransform\n",
    "from LARS import LARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb1838",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO\n",
    "-->> It might be interesting investigate the efficiency frontier between max_batch and views \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "warmup_steps = 10 \n",
    "start_epoch = 1\n",
    "epochs = 100 # Original set to 1000 \n",
    "output_enc = 512\n",
    "dim = 8192\n",
    "num_views = 4\n",
    "offset = 1 \n",
    "\n",
    "num_workers = 2\n",
    "device = 'cuda' # or 'cuda' for faster training\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "# VicREG\n",
    "base_lr_ = 0.2\n",
    "lr_head = 0.3\n",
    "learning_rate = batch_size/256 * base_lr_ \n",
    "weight_decay = 1e-6\n",
    "\n",
    "logging.basicConfig(filename=f'logs/b:{batch_size}.log', filemode='w', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79dd934",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "#trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)  \n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)  \n",
    "trainset.transform = ViewTransform(2)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09abaec",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e24aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projector():\n",
    "    proj_layers = []\n",
    "    proj_layers.append(torch.nn.Flatten())\n",
    "\n",
    "    proj_layers.append(torch.nn.Linear(output_enc, dim))\n",
    "    proj_layers.append(torch.nn.ReLU(dim))\n",
    "    proj_layers.append(torch.nn.BatchNorm1d(dim))\n",
    "\n",
    "    proj_layers.append(torch.nn.Linear(dim, dim))\n",
    "    proj_layers.append(torch.nn.ReLU(dim))\n",
    "    proj_layers.append(torch.nn.BatchNorm1d(dim))\n",
    "    \n",
    "    proj_layers.append(torch.nn.Linear(dim, dim, bias=False))\n",
    "    \n",
    "    return torch.nn.Sequential(*proj_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d8a5c7",
   "metadata": {},
   "source": [
    "## VicREG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f33d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VicReg Paper - with modifications\n",
    "def VIC_Reg(Z):\n",
    "    \n",
    "    N = Z[0].shape[0]\n",
    "    D = Z[0].shape[1]\n",
    "\n",
    "    mu = 25\n",
    "    la = 25\n",
    "    nu = 1 \n",
    "    \n",
    "    sim_loss = 0\n",
    "    std_loss = 0\n",
    "    cov_loss = 0\n",
    "\n",
    "    for i in range(len(Z)): \n",
    "        for j in range(i+1, len(Z)): \n",
    "            sim_loss += F.mse_loss(Z[i], Z[j])\n",
    "\n",
    "    for zi in Z: \n",
    "        std_zi = torch.sqrt(zi.var(dim=0) + 1e-04)\n",
    "        std_loss += torch.mean(torch.relu(1 - std_zi)) \n",
    "    \n",
    "    for zi in Z: \n",
    "        zi = zi - zi.mean(dim=0)\n",
    "        cov_zi = (zi.T @ zi) / (N - 1)\n",
    "        cov_zi = cov_zi[~torch.eye(cov_zi.shape[0], dtype=bool,device=device)]\n",
    "        cov_loss += cov_zi.pow_(2).sum() / D\n",
    "\n",
    "    sim_loss /= (len(Z) * (len(Z)-1)) / 2\n",
    "    std_loss /= len(Z)\n",
    "    cov_loss /= len(Z)\n",
    "    \n",
    "    logging.info('IL: %.3f, STDL: %.3f, CVL: %.3f',la * sim_loss, mu * std_loss, nu * cov_loss)\n",
    "\n",
    "    loss = la * sim_loss + mu * std_loss + nu * cov_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
    "\n",
    "def adjust_learning_rate(optimizer, loader, step):\n",
    "    max_steps = epochs * len(loader)\n",
    "    warmup_steps = 10 * len(loader)\n",
    "    base_lr = base_lr_ * batch_size / 256\n",
    "    if step < warmup_steps:\n",
    "        lr = base_lr * step / warmup_steps\n",
    "    else:\n",
    "        step -= warmup_steps\n",
    "        max_steps -= warmup_steps\n",
    "        q = 0.5 * (1 + math.cos(math.pi * step / max_steps))\n",
    "        end_lr = base_lr * 0.001\n",
    "        lr = base_lr * q + end_lr * (1 - q)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ccb0d4",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainset, offset, batch_size=2048, load_from_checkpoint=\"\"): \n",
    "\n",
    "    encoder = torchvision.models.resnet18() # also try with pretrained=true\n",
    "    encoder.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    encoder = torch.nn.Sequential(*list(encoder.children())[:-1])\n",
    "\n",
    "    \n",
    "    rpoj = projector()\n",
    "    model = torch.nn.Sequential(encoder, rpoj)\n",
    "    model.train()\n",
    "    optimizer = LARS(model.parameters(),lr=0,weight_decay=weight_decay)\n",
    "    \n",
    "    if load_from_checkpoint != \"\": \n",
    "        checkpoint = torch.load(load_from_checkpoint)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        #start_epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss'] \n",
    "        model.train()\n",
    "        \n",
    "    model = model.to(device)\n",
    "    last_time = time.time()\n",
    "\n",
    "    for i in range(start_epoch, epochs+1):\n",
    "        losses = []\n",
    "        for step, (X, _) in tqdm(enumerate(trainloader), total=epochs):\n",
    "            \n",
    "            if step % offset == 0: \n",
    "                lr = adjust_learning_rate(optimizer, trainloader, step)  \n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            Z = []\n",
    "            for xi in X: \n",
    "                xi = xi.to(device)\n",
    "                Z.append(model(xi))\n",
    "                \n",
    "            loss = VIC_Reg(Z)\n",
    "            logging.info('%s ,Epoch: %d, Step: %d, Loss: %.3f, Elapsed: %d, View: %d', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())), i, step, loss, time.time()-last_time, len(Z))\n",
    "            \n",
    "            if step % offset == 0: \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.detach().item())\n",
    "\n",
    "            last_time = time.time()\n",
    "\n",
    "\n",
    "        print(f\"Epoch: {i}, loss: {np.mean(losses)}\")\n",
    "        #DL 1 Homework 1 \n",
    "\n",
    "        if i%10==0:\n",
    "            torch.save({\n",
    "                'epoch': i,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': np.mean(losses),\n",
    "                }, f'model_{num_views}_{batch_size}_epoch_{i}.pt')\n",
    "    \n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff997348",
   "metadata": {},
   "source": [
    "### Fixed Batch - Increasing Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c08dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [2,4,6,8]:\n",
    "    print(f\"View: {i} / {num_views}\")\n",
    "    view = i \n",
    "    at_limit = False\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)  \n",
    "    trainset.transform = ViewTransform(view)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    #\n",
    "    #logging.basicConfig(filename=f'logs/loss_details_b:{batch_size}.log', filemode='w', level=logging.DEBUG)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            #if(at_limit):\n",
    "            #    batch_size //= 2\n",
    "            #    offset = offset * 2\n",
    "            #    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "                \n",
    "            encoder_vicreg = train(trainloader, offset)\n",
    "            break  #  successful break out of the loop\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(\"Out of memory error occurred. Reducing batch size and retrying...\")\n",
    "                # Reduce batch size & Conversly increase step size\n",
    "                batch_size //= 2\n",
    "                offset = offset * 2\n",
    "                at_limit = True\n",
    "\n",
    "                trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a20f8d",
   "metadata": {},
   "source": [
    "### Fixed Total Gradients - Increase Views & Reduce Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5294e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [4,6,8]:\n",
    "    print(f\"View: {i} / {num_views}\")\n",
    "    view = i \n",
    "    at_limit = False\n",
    "    \n",
    "    batch_size = 2 * batch_size / i \n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)  \n",
    "    trainset.transform = ViewTransform(view)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    #\n",
    "    #logging.basicConfig(filename=f'logs/loss_details_b:{batch_size}.log', filemode='w', level=logging.DEBUG)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            #if(at_limit):\n",
    "            #    batch_size //= 2\n",
    "            #    offset = offset * 2\n",
    "            #    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "                \n",
    "            encoder_vicreg = train(trainloader, offset, batch_size)\n",
    "            break  #  successful break out of the loop\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(\"Out of memory error occurred. Reducing batch size and retrying...\")\n",
    "                # Reduce batch size & Conversly increase step size\n",
    "                batch_size //= 2\n",
    "                offset = offset * 2\n",
    "                at_limit = True\n",
    "\n",
    "                trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea06eb0",
   "metadata": {},
   "source": [
    "## Model Evaluations - Beyond Loss (kNN, LinearHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a33392",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(32),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models(path=\"./models/\"):\n",
    "    return glob.glob(path+\"*.pt\")\n",
    "\n",
    "def load_final_models(path=\"./models/\"):\n",
    "    return glob.glob(path+\"*100*.pt\")\n",
    "# ChatGPT &/ PyTorch topK \n",
    "def top_k_accuracy(output, target, k=1):\n",
    "    _, pred = output.topk(k, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    correct_k = correct[:k].reshape(-1).float().sum()\n",
    "    accuracy = correct_k.mul_(100.0 / target.size(0))\n",
    "    return accuracy.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_path in tqdm(load_all_models(), total=len(load_all_models())): \n",
    "\n",
    "    encoder = torchvision.models.resnet18()\n",
    "    encoder.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    encoder = torch.nn.Sequential(*list(encoder.children())[:-1])\n",
    "\n",
    "    checkpoint = torch.load(model_path)\n",
    "    encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    encoder.eval()\n",
    "    encoder.to(device)\n",
    "    \n",
    "    X_train_embedding = []\n",
    "    X_test_embedding = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "\n",
    "    for x,y in train_loader:\n",
    "        x = x.to(device)\n",
    "        X_train_embedding.append(encoder(x))\n",
    "        y_train.append(y_train)\n",
    "\n",
    "    for x,y in test_loader:\n",
    "        x = x.to(device)\n",
    "        X_test_embedding.append(encoder(x))\n",
    "        y_test.append(y_train)\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train_embedding, y_train)\n",
    "    X_test_predicted = knn.predict(X_test_embedding)\n",
    "    accuracy = accuracy_score(y_test, X_test_predicted)\n",
    "    logging.info(\"Model: %s, kNN-Accuracy: %.3f\", model_path, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_path in tqdm(load_final_models(), total=epochs*len(load_final_models())):\n",
    "    \n",
    "    encoder = torchvision.models.resnet18()\n",
    "    encoder.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    encoder.requires_grad_(False)\n",
    "    encoder.eval()\n",
    "    encoder.to(device)\n",
    "\n",
    "    head = torch.nn.Linear(output_enc, num_classes)\n",
    "    head.weight.data.normal_(mean=0.0, std=0.01)\n",
    "    head.bias.data.zero_()\n",
    "    model = torch.nn.Sequential(encoder, head)\n",
    "\n",
    "    param_groups = [dict(params=head.parameters(), lr=lr_head)]\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.SGD(param_groups, 0, momentum=0.9, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        for step, (X, Y) in tqdm(enumerate(trainloader), total=epochs):\n",
    "            X = X.to(device)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                accuracy = []\n",
    "                for images, targets in test_loader: \n",
    "                    output = model(images)\n",
    "                    accuracy.append(top_k_accuracy(output, targets))\n",
    "                \n",
    "                accuracy = accuracy.mean()\n",
    "            \n",
    "            logging.info(\"Model: %s, Accuracy: %.3f, Epoch: %d\", model_path, accuracy, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68c1bd",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0693a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"logs/b_2048_v_2.log\", delimiter=',', header=None)\n",
    "columns = [\"Info\", \"Epoch\", \"Step\", \"Loss\", \"Elapsed\", \"Views\"]\n",
    "df.columns = columns\n",
    "df[\"Epoch\"] = df[\"Epoch\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "df[\"Step\"] = df[\"Step\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "df[\"Loss\"] = df[\"Loss\"].str.extract(r\"(\\d+\\.\\d+)\", expand=False).astype(float)\n",
    "df[\"Elapsed\"] = df[\"Elapsed\"].str.extract(r\"(\\d+)\", expand=False).astype(float)\n",
    "df[\"Views\"] = df[\"Views\"].str.extract(r\"(\\d+)\", expand=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fc2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    grouped_data = df[df[\"Views\"] == i+2]\n",
    "    #grouped_data = grouped_data.groupby(\"Epoch\").agg({\"Loss\": \"mean\"}).reset_index()\n",
    "    ax.plot(grouped_data[\"Loss\"])\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"View {i+2}\")\n",
    "\n",
    "for i in range(2,num_views+1): \n",
    "    grouped_data = df[df[\"Views\"] == i]\n",
    "    #grouped_data.groupby(\"Epoch\").agg({\"Loss\": \"mean\"}).reset_index()\n",
    "    ax.plot(grouped_data[\"Epoch\"], grouped_data[\"Loss\"],label=f\"{i} Views\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Average Loss\")\n",
    "\n",
    "\n",
    "# Get the handles and labels from the plots\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.figlegend(handles, labels, loc='center', bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "for view in range(2,num_views+1):\n",
    "    last_loss = df[df[\"Views\"] == view][\"Loss\"].tail(1).values[0]\n",
    "    print(f\"Last loss for View {view}: {last_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b0c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"logs/b_128_v_2.log\", delimiter=',', header=None)\n",
    "columns = [\"Info\", \"Epoch\", \"Step\", \"Loss\", \"Elapsed\", \"Views\"]\n",
    "df.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"logs/b_128_v_2.log\", delimiter=',', header=None)\n",
    "columns = [\"Info\", \"Epoch\", \"Step\", \"Loss\", \"Elapsed\", \"Views\"]\n",
    "df.columns = columns\n",
    "df[\"Epoch\"] = df[\"Epoch\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "df[\"Step\"] = df[\"Step\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n",
    "df[\"Loss\"] = df[\"Loss\"].str.extract(r\"(\\d+\\.\\d+)\", expand=False).astype(float)\n",
    "df[\"Elapsed\"] = df[\"Elapsed\"].str.extract(r\"(\\d+)\", expand=False).astype(float)\n",
    "df[\"Views\"] = df[\"Views\"].str.extract(r\"(\\d+)\", expand=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fcf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    grouped_data = df[df[\"Views\"] == i+2]\n",
    "    #grouped_data = grouped_data.groupby(\"Epoch\").agg({\"Loss\": \"mean\"}).reset_index()\n",
    "    ax.plot(grouped_data[\"Loss\"])\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"View {i+2}\")\n",
    "\n",
    "for i in range(2,3+1): \n",
    "    grouped_data = df[df[\"Views\"] == i]\n",
    "    #grouped_data.groupby(\"Epoch\").agg({\"Loss\": \"mean\"}).reset_index()\n",
    "    ax.plot(grouped_data[\"Epoch\"], grouped_data[\"Loss\"],label=f\"{i} Views\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Average Loss\")\n",
    "\n",
    "\n",
    "# Get the handles and labels from the plots\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.figlegend(handles, labels, loc='center', bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
