{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1b63f5-2b84-438d-803f-e996f8a346a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from lightly.data import LightlyDataset, SimCLRCollateFunction, collate\n",
    "from lightly.loss import NTXentLoss\n",
    "from lightly.models import ResNetGenerator\n",
    "from lightly.models.modules.heads import MoCoProjectionHead\n",
    "from lightly.models.utils import (\n",
    "    batch_shuffle,\n",
    "    batch_unshuffle,\n",
    "    deactivate_requires_grad,\n",
    "    update_momentum,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80f7d6f-f39f-4797-bf27-6ad5754aa592",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "batch_size = 512\n",
    "memory_bank_size = 4096\n",
    "seed = 1\n",
    "max_epochs = 1\n",
    "input_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86bc2af-bd90-4801-bf78-c418e856b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = \"../Datasets/cifar10/train/\"\n",
    "path_to_test = \"../Datasets/cifar10/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4740f441-2fe0-42a1-9cf9-d7ec52082e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25212e21-6cb6-45ad-931d-0a132c628699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoCo v2 uses SimCLR augmentations, additionally, disable blur\n",
    "collate_fn = SimCLRCollateFunction(input_size=input_size, vf_prob=0.5, rr_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bff137f-e615-49b7-b4f4-7e0ebdba5541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentations typically used to train on cifar-10\n",
    "train_classifier_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomCrop(input_size, padding=4),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=collate.imagenet_normalize[\"mean\"],\n",
    "            std=collate.imagenet_normalize[\"std\"],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# No additional augmentations for the test set\n",
    "test_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize((input_size, input_size)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=collate.imagenet_normalize[\"mean\"],\n",
    "            std=collate.imagenet_normalize[\"std\"],\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dc1717d-8298-4dbc-9df4-a1536a122468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We use the moco augmentations for training moco\n",
    "dataset_train_moco = LightlyDataset(input_dir=path_to_train)\n",
    "\n",
    "# Since we also train a linear classifier on the pre-trained moco model we\n",
    "# reuse the test augmentations here (MoCo augmentations are very strong and\n",
    "# usually reduce accuracy of models which are not used for contrastive learning.\n",
    "# Our linear layer will be trained using cross entropy loss and labels provided\n",
    "# by the dataset. Therefore we chose light augmentations.)\n",
    "dataset_train_classifier = LightlyDataset(\n",
    "    input_dir=path_to_train, transform=train_classifier_transforms\n",
    ")\n",
    "\n",
    "dataset_test = LightlyDataset(input_dir=path_to_test, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1ce14-17af-4496-bef5-7d86f0c7c140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7bedd94-3865-466c-a63e-364a845aae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_moco = torch.utils.data.DataLoader(\n",
    "    dataset_train_moco,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "dataloader_train_classifier = torch.utils.data.DataLoader(\n",
    "    dataset_train_classifier,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66314f43-1227-4d38-b431-040d41201baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MocoModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\", 1, num_splits=8)\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1],\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "\n",
    "        # create a moco model based on ResNet\n",
    "        self.projection_head = MoCoProjectionHead(512, 512, 128)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        deactivate_requires_grad(self.backbone_momentum)\n",
    "        deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        # create our loss with the optional memory bank\n",
    "        self.criterion = NTXentLoss(temperature=0.1, memory_bank_size=memory_bank_size)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x_q, x_k), _, _ = batch\n",
    "\n",
    "        # update momentum\n",
    "        update_momentum(self.backbone, self.backbone_momentum, 0.99)\n",
    "        update_momentum(self.projection_head, self.projection_head_momentum, 0.99)\n",
    "\n",
    "        # get queries\n",
    "        q = self.backbone(x_q).flatten(start_dim=1)\n",
    "        q = self.projection_head(q)\n",
    "\n",
    "        # get keys\n",
    "        k, shuffle = batch_shuffle(x_k)\n",
    "        k = self.backbone_momentum(k).flatten(start_dim=1)\n",
    "        k = self.projection_head_momentum(k)\n",
    "        k = batch_unshuffle(k, shuffle)\n",
    "\n",
    "        loss = self.criterion(q, k)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    #def on_train_epoch_end(self):\n",
    "     #   self.custom_histogram_weights()\n",
    "\n",
    "    # We provide a helper method to log weights in tensorboard\n",
    "    # which is useful for debugging.\n",
    "  #  def custom_histogram_weights(self):\n",
    "    #    for name, params in self.named_parameters():\n",
    "    #        self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=6e-2,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fb2a374-27e1-424b-bc9e-dc758d005ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        # use the pretrained ResNet backbone\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # freeze the backbone\n",
    "        deactivate_requires_grad(backbone)\n",
    "\n",
    "        # create a linear layer for our downstream classification model\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.output = []\n",
    "    def forward(self, x):\n",
    "        y_hat = self.backbone(x).flatten(start_dim=1)\n",
    "        y_hat = self.fc(y_hat)\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss_fc\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.nn.functional.softmax(y_hat, dim=1)\n",
    "\n",
    "        # calculate number of correct predictions\n",
    "        _, predicted = torch.max(y_hat, 1)\n",
    "        num = predicted.shape[0]\n",
    "        correct = (predicted == y).float().sum()\n",
    "        self.output += [num, correct]\n",
    "        return num, correct\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # calculate and log top1 accuracy\n",
    "        outputs=self.output\n",
    "        if outputs:\n",
    "            total_num = 0\n",
    "            total_correct = 0\n",
    "            for res in outputs:\n",
    "                num=res[0]\n",
    "                correct=res[1]\n",
    "                total_num += num\n",
    "                total_correct += correct\n",
    "            acc = total_correct / total_num\n",
    "            self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(self.fc.parameters(), lr=30.0)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f9f2181-d66e-40cc-a33a-81559ca7fe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                     | Type               | Params\n",
      "----------------------------------------------------------------\n",
      "0 | backbone                 | Sequential         | 11.2 M\n",
      "1 | projection_head          | MoCoProjectionHead | 328 K \n",
      "2 | backbone_momentum        | Sequential         | 11.2 M\n",
      "3 | projection_head_momentum | MoCoProjectionHead | 328 K \n",
      "4 | criterion                | NTXentLoss         | 0     \n",
      "----------------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "11.5 M    Non-trainable params\n",
      "23.0 M    Total params\n",
      "91.977    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 97/97 [01:10<00:00,  1.38it/s, v_num=7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████| 97/97 [01:11<00:00,  1.36it/s, v_num=7]\n"
     ]
    }
   ],
   "source": [
    "model = MocoModel()\n",
    "trainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\n",
    "trainer.fit(model, dataloader_train_moco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5e331-85d0-4266-a65d-4a9984ecfb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | backbone  | Sequential       | 11.2 M\n",
      "1 | fc        | Linear           | 5.1 K \n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "classifier = Classifier(model.backbone)\n",
    "trainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\n",
    "trainer.fit(classifier, dataloader_train_classifier, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030475b-c05e-4b32-a410-19b8e8251b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdbda431-5f7e-48dc-86e2-8e1f89997336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabbd621-b36b-4f10-9dc0-e70415f72190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
